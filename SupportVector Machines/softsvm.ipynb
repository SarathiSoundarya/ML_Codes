{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtDikVRIImaMnX7A9Co2M5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Question 3**\n","Generate Test Datasets in Python with scikit-learn (sklearn), specifically to gener\n","ate blobs of points with a Gaussian distribution (n_samples=1000, n_features=2,\n","\n","centers=2, cluster_std=1.05, random_state=2). Use all training data for those bi-\n","nary pair of points to learn two binary classifiers using linear SVM and nonlinear SVM (with Gaussian RBF kernel), and compare and discuss the performance\n","\n","and efficiency of the linear SVM and nonlinear SVM methods."],"metadata":{"id":"6a7IbjdnuPW5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYoIDkG1coA9"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","source":["**Soft Linear SVMs** <br />\n","$d$ features, $n$ points <br />\n","data points: $(x_{i},y_{i})$<br />\n","Hinge loss for the $i^{th}$ datapoint $l_{i}=max(0,1-(y_{i}(w\\cdot x_{i}+b))$<br />\n","Cost function to minimize $J=\\frac{1}{2}||w||^{2}+\\frac{1}{n}\\sum_{i=1}^{n}l_{i}$  <br />\n","Note that $l_{i}$ is zero for $y_{i}f(x_{i})>1$ where $f(x_{i})=w\\cdot x_{i}+b$<br /><br />\n","**Derivatives**: <br />\n","If $y_{i}\\cdot f(x_{i})\\geq 1 $,<br />\n","$\\frac{\\partial J}{∂ w_{j}}=w_{j}-\\frac{1}{n}\\sum_{y_{i}\\cdot f(x_{i})<1}y_{i}x_{j}^{i}$ <br />\n","$\\frac{∂ J}{\\partial b}=-\\frac{1}{n}\\sum_{y_{i}\\cdot f(x_{i})<1}^{n}y_{i}$<br /><br />\n","**Update Rule**<br />\n","$w_{j}=w_{j}-\\alpha\\frac{∂ J}{∂ w_{j}} $<br />\n","$b=b-\\alpha\\frac{∂ J}{\\partial b}$\n","\n","The below code is a vectorized implementation of the SVM."],"metadata":{"id":"3IrTNyJW5bME"}},{"cell_type":"code","source":["class linearsvm:\n","    def __init__(self, learning_rate=0.01, n_iters=1000):\n","        self.lr = learning_rate\n","        self.n_iters = n_iters\n","        self.weights = None\n","        self.bias = 0\n","\n","    def gradient_descent(self,x,y,y_pred):\n","      n_samples,n_features=x.shape\n","      con=y*y_pred\n","      dw=np.zeros((1,n_features))\n","      for i in range(n_features):\n","        dw[0][i]=self.weights[0][i]-(1/n_samples)*np.sum(np.where(con>=1,0,1)*y*(x.T[i])) \n","      db=-(1/n_samples)*np.sum(np.where(con>=1,0,1)*y)\n","      return dw,db\n","    #the np.where condition takes care to account for the places where the hinge loss <1 and sum up only those places\n","\n","    def update(self,dw,db):\n","      self.weights-=self.lr*dw\n","      self.bias-=self.lr*db\n","\n","    def fit(self, x, y,xtest,ytest):\n","      #initializations\n","      cost_list=[] #for training data\n","      accuracy=[]\n","      n_samples,n_features=x.shape\n","      y=np.where(y==0,-1,1)\n","      ytest=np.where(ytest==0,-1,1)\n","      ytest=ytest.reshape((1,xtest.shape[0]))\n","      self.weights=np.zeros((1,n_features))\n","\n","\n","      for i in range(self.n_iters):\n","        #gradient descent and updation\n","        y_predict=np.dot(self.weights,x.T)+self.bias\n","        cost=(1/2*n_samples)*(np.sum(np.square(y-y_predict)))\n","        dw,db=self.gradient_descent(x,y,y_predict)\n","        self.update(dw,db)\n","\n","        #plotting at every 100th iteration \n","        if(i%100==0):\n","          a_max = np.amax(x[:,0])\n","          a_min = np.amin(x[:,0])\n","          xpts = np.linspace(a_min,a_max)\n","          ypts = -(self.weights[0][0]*xpts + self.bias)/self.weights[0][1]\n","          ax.plot(xpts, ypts,label=i)\n","          plt.legend()\n","        \n","        #testing the accuracy on the test data\n","        y_p=np.dot(self.weights,xtest.T)+self.bias\n","        y_p=np.where(y_p>0,1,-1)\n","        acc=np.count_nonzero(ytest==y_p)/xtest.shape[0]\n","        accuracy.append(acc)\n","        cost_list.append(cost)\n","      return cost_list,accuracy\n"],"metadata":{"id":"pvEhkgAZCqcJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x,y=datasets.make_blobs(n_samples=1000, n_features=2,centers=2, cluster_std=1.05, random_state=2)\n","xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2)\n","#fig, ax = plt.subplots()\n","linsvm=linearsvm(0.01,500)\n","#plotting\n","fig, ax = plt.subplots()\n","plt.scatter(xtrain[:,0], xtrain[:,1],marker=\"o\", c=ytrain)\n","\n","#linear svm\n","cost_list,accuracy=linsvm.fit(xtrain,ytrain,xtest,ytest)\n","print('costlist on train data at every iteration: ',cost_list)\n","print(\"weights,bias:\",linsvm.weights,linsvm.bias)\n","print('accuracy on test data at every iteration:', accuracy)\n","\n","#plot the cost and accuracy\n","plt.title('Hyperplane produced by Soft SVM using Linear Kernel at every iteration')\n","plt.show()\n","plt.plot(cost_list,'.')\n","plt.xlabel('iteration')\n","plt.ylabel('cost on train data')\n","plt.title('Cost function for SoftSVM on training data')\n","plt.show()\n","plt.plot(accuracy,'.')\n","plt.xlabel('iteration')\n","plt.ylabel('accuracy on test data for Soft SVM')\n","plt.show()\n","#observe that the cost increases after certain iterations because it starts overfitting the training data\n"],"metadata":{"id":"lf9XruCcumj6","executionInfo":{"status":"error","timestamp":1669972595459,"user_tz":-330,"elapsed":1640,"user":{"displayName":"SOUNDARYA S","userId":"16308354762737564396"}},"colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"0a877ae9-806e-48b1-8764-d6a8a9dad9c4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3ac650c89a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_blobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#fig, ax = plt.subplots()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlinsvm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinearsvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"]}]}]}